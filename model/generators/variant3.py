import tensorflow.compat.v1 as tf
import tensorflow_compression as tfc

from model.generators.transforms.autoregressive import (
    AnalysisTransform,
    SynthesisTransform,
    HyperAnalysisTransform,
    HyperSynthesisTransform,
    EntropyParameters,
    ContextModel,
)
from dnc.utils import *
from model.utils import HParams


def generator(
    params,
    inputs,
    is_training,
    predict: bool = False,
    decompression: bool = False,
    decompression_vars=None,
):
    """
        Builds the generator of the model

    :param params: (dict) hyperparameters of the model read from a JSON file
    :param inputs: (tf.Tensor) input image to the generator
    :param is_training: (bool) if True, return training graph of generator
    :param predict: (bool) only available when `is_training` is false i.e evaluation graph.
        This mode has been used for the predictor EstimatorSpec, compression and benchmark modes of the model
    :param decompression: (bool) special graph mode for decompression of compressed format
        generated by the above prediction mode. If `True` predict is unavailable
    :param decompression_vars: (dict) takes in a dictionary of tensors needed by decompression
        graph for decompressing the compressed format
    """
    # check inputs
    print("//////////// GENERATOR_inputs : ", inputs)
    # assert inputs.get_shape().as_list()[-3:] == [params.patchsize, params.patchsize, 3]
    x = inputs
    n_recurrence = 1 # number of loops for applying context model

    # define layers of generator
    # Instantiate the generator
    encoder = AnalysisTransform(params.num_filters)
    decoder = SynthesisTransform(params.num_filters)
    hyper_encoder = HyperAnalysisTransform(params.num_filters)
    hyper_decoder = HyperSynthesisTransform(params.num_filters)
    entropy_parameters = EntropyParameters(params.num_filters)
    context_model = ContextModel(params.num_filters)
    # context_model = functools.partial(
    #     _base_noup_smallkey_spec,
    #     h=None,
    #     init=False,
    #     ema=None,
    #     dropout_p=0.5,
    #     nr_resnet=2,
    #     nr_filters=params.num_filters,
    #     attn_rep=4,
    #     output_units=int(params.num_filters * 2.0),
    #     att_downsample=1,
    #     resnet_nonlinearity="concat_elu",
    # )
    scale_table = np.exp(
        np.linspace(
            np.log(params.scales_min),
            np.log(params.scales_max),
            params.scales_levels,
        )
    )

    if is_training:
        entropy_bottleneck = tfc.EntropyBottleneck()
        with tf.variable_scope("generator"):
            y = encoder(x)
            z = hyper_encoder(abs(y))
            z_tilde, z_likelihoods = entropy_bottleneck(z, training=True)
            psi = hyper_decoder(z_tilde)
            ep_in = tf.concat([psi, psi], axis=3)

            for i in range(n_recurrence+1):
                tf.logging.info("====<< Model in ContextModel recurrence loop iteration : %s >>====" % (i))
                mu, sigma = entropy_parameters(ep_in)
                conditional_bottleneck = tfc.GaussianConditional(
                    scale=sigma, mean=mu, scale_table=scale_table
                )
                y_tilde, y_likelihoods = conditional_bottleneck(y, training=True)

                if i == n_recurrence: break
                tf.logging.info("====<< Calculating 'phi (ContextModel)' >>====")
                phi = context_model(y_tilde)
                ep_in = tf.concat([phi, psi], axis=3)

            x_tilde = decoder(y_tilde)

            num_pixels = params.batchsize * params.patchsize ** 2

            # Total number of bits divided by number of pixels.
            train_bpp = (
                tf.reduce_sum(tf.log(y_likelihoods))
                + tf.reduce_sum(tf.log(z_likelihoods))
            ) / (-np.log(2) * num_pixels)

        # with tf.name_scope("generatorTRAIN/"):
        #     tf.summary.scalar("bpp", train_bpp)
        #
        #     tf.summary.image("x", quantize_image(x))
        #     tf.summary.image("x_tilde", quantize_image(x_tilde))

        genx = x_tilde
        bpp = train_bpp

        return genx, bpp, entropy_bottleneck

    else:
        if decompression:
            entropy_bottleneck = tfc.EntropyBottleneck(dtype=tf.float32)

            string = decompression_vars["string"]
            side_string = decompression_vars["side_string"]
            z_shape = decompression_vars["z_shape"]
            y_shape = decompression_vars["y_shape"]

            with tf.variable_scope("generator"):
                z_hat = entropy_bottleneck.decompress(
                    side_string, z_shape, channels=params.num_filters
                )
                psi = hyper_decoder(z_hat)
                ep_in = tf.concat([psi, psi], axis=3)

                for i in range(n_recurrence+1):
                    tf.logging.info("====<< Model in ContextModel recurrence loop iteration : %s >>====" % (i))
                    mu, sigma = entropy_parameters(ep_in)
                    sigma = sigma[:, : y_shape[0], : y_shape[1], :]
                    mu = mu[:, : y_shape[0], : y_shape[1], :]
                    conditional_bottleneck = tfc.GaussianConditional(
                        scale=sigma, mean=mu, scale_table=scale_table, dtype=tf.float32
                    )
                    y_hat = conditional_bottleneck.decompress(string)

                    if i == n_recurrence: break
                    tf.logging.info("====<< Calculating 'phi (ContextModel)' >>====")
                    phi = context_model(y_hat)
                    ep_in = tf.concat([phi, psi], axis=3)

                x_hat = decoder(y_hat)

            return x_hat, y_hat, z_hat, sigma

        else:
            entropy_bottleneck = tfc.EntropyBottleneck()
            # Transform and compress the image.
            x_shape = tf.shape(x)
            with tf.variable_scope("generator"):
                y = encoder(x)
                y_shape = tf.shape(y)
                y_hat_cm, y_likelihoods_cm = entropy_bottleneck(y, training=False)
                z = hyper_encoder(abs(y))
                # here z_hat is the hyper-latent code
                z_hat, z_likelihoods = entropy_bottleneck(z, training=False)
                psi = hyper_decoder(z_hat)
                ep_in = tf.concat([psi, psi], axis=3)

                for i in range(n_recurrence+1):
                    tf.logging.info("====<< Model in ContextModel recurrence loop iteration : %s >>====" % (i))
                    mu, sigma = entropy_parameters(ep_in)
                    if predict:
                        sigma = sigma[:, : y_shape[1], : y_shape[2], :]
                        mu = mu[:, : y_shape[1], : y_shape[2], :]

                    conditional_bottleneck = tfc.GaussianConditional(
                        scale=sigma, mean=mu, scale_table=scale_table
                    )
                    # Transform the quantized image back (if requested).
                    # here y_hat is the latent code
                    y_hat, y_likelihoods = conditional_bottleneck(y, training=False)

                    if i == n_recurrence: break
                    tf.logging.info("====<< Calculating 'phi (ContextModel)' >>====")
                    phi = context_model(y_hat)
                    ep_in = tf.concat([phi, psi], axis=3)

                x_hat = decoder(y_hat)
                if predict:
                    # During prediction mode output important tensors for model analysis
                    gen_metadata = dict()
                    gen_metadata["y"] = y
                    gen_metadata["y_shape"] = y_shape
                    gen_metadata["z"] = z
                    gen_metadata["z_hat"] = z_hat
                    gen_metadata["z_likelihoods"] = z_likelihoods
                    gen_metadata["sigma"] = sigma
                    gen_metadata["mu"] = mu
                    gen_metadata["y_hat"] = y_hat
                    gen_metadata["y_likelihoods"] = y_likelihoods

                    # analysis of hyper-latent code
                    z_hat_shape = tf.shape(z_hat)
                    z_ll_shape = tf.shape(z_likelihoods)
                    z_hat_slice = z_hat[0, : z_hat_shape[1], : z_hat_shape[2], :]
                    z_ll_slice = z_likelihoods[0, : z_ll_shape[1], : z_ll_shape[2], :]
                    hyperlatent_entropy = tf.math.multiply(tf.log(z_ll_slice), -1)
                    hle_shape = tf.shape(hyperlatent_entropy)

                    # calculate hyper-latent entropy per channel dimension
                    hyperlatent_entropy_pc = tf.reduce_sum(
                        hyperlatent_entropy, axis=[0, 1]
                    )
                    hle_pc_shape = tf.shape(hyperlatent_entropy_pc)
                    maxhl_ent_loc = tf.math.argmax(
                        hyperlatent_entropy_pc, name="maxhl_ent_loc"
                    )
                    maxhl_ent_val = tf.reduce_max(hyperlatent_entropy_pc)
                    z_hat_slice_maxhle = z_hat_slice[:, :, maxhl_ent_loc]
                    hle_max_slice = hyperlatent_entropy[:, :, maxhl_ent_loc]

                    # get predicted scales for analysis
                    pred_scale_img = sigma[0, : y_shape[1], : y_shape[2], :]
                    sigma_slice = pred_scale_img[:, :, maxhl_ent_loc]
                    pred_mean_img = mu[0, : y_shape[1], : y_shape[2], :]
                    mu_slice = pred_mean_img[:, :, maxhl_ent_loc]

                    # analysis of latent codes
                    y_hat_shape = tf.shape(y_hat)
                    y_ll_shape = tf.shape(y_likelihoods)
                    y_hat_slice = y_hat[0, : y_hat_shape[1], : y_hat_shape[2], :]
                    y_ll_slice = y_likelihoods[0, : y_ll_shape[1], : y_ll_shape[2], :]
                    latent_entropy = tf.math.multiply(tf.log(y_ll_slice), -1)
                    le_shape = tf.shape(latent_entropy)
                    # calculate latent entropy per channel
                    latent_entropy_pc = tf.reduce_sum(latent_entropy, axis=[0, 1])
                    le_pc_shape = tf.shape(latent_entropy_pc)
                    maxl_ent_loc = tf.math.argmax(
                        latent_entropy_pc, name="maxl_ent_loc"
                    )
                    maxl_ent_val = tf.reduce_max(latent_entropy_pc)
                    y_hat_slice_maxle = y_hat_slice[:, :, maxl_ent_loc]
                    le_max_slice = latent_entropy[:, :, maxl_ent_loc]

                    gen_metadata["maxhl_ent_loc"] = maxhl_ent_loc
                    gen_metadata["maxhl_ent_val"] = maxhl_ent_val
                    gen_metadata["maxl_ent_loc"] = maxl_ent_loc
                    gen_metadata["maxl_ent_val"] = maxl_ent_val
                    gen_metadata["hyperlatent_entropy_pc"] = hyperlatent_entropy_pc
                    gen_metadata["latent_entropy_pc"] = latent_entropy_pc
                    gen_metadata["hle_shape"] = hle_shape
                    gen_metadata["hle_pc_shape"] = hle_pc_shape
                    gen_metadata["le_shape"] = le_shape
                    gen_metadata["le_pc_shape"] = le_pc_shape
                    gen_metadata["z_hat_slice_maxhle"] = z_hat_slice_maxhle
                    gen_metadata["y_hat_slice_maxle"] = y_hat_slice_maxle
                    gen_metadata["hle_max_slice"] = hle_max_slice
                    gen_metadata["le_max_slice"] = le_max_slice
                    gen_metadata["sigma_slice"] = sigma_slice
                    gen_metadata["mu_slice"] = mu_slice

                    # Remove batch dimension, and crop away any extraneous padding on the bottom
                    # or right boundaries.
                    x_hat_img = x_hat[0, : x_shape[1], : x_shape[2], :]
                    x_hat = x_hat[:, : x_shape[1], : x_shape[2], :]
                    side_string = entropy_bottleneck.compress(z)
                    string = conditional_bottleneck.compress(y)
                    encoding_tensors = [
                        string,
                        side_string,
                        tf.shape(x)[1:-1],
                        tf.shape(y)[1:-1],
                        tf.shape(z)[1:-1],
                    ]

                num_pixels = tf.cast(tf.reduce_prod(tf.shape(x)[:-1]), dtype=tf.float32)

                # Total number of bits divided by number of pixels.
                eval_bpp = (
                    tf.reduce_sum(tf.log(y_likelihoods))
                    + tf.reduce_sum(tf.log(z_likelihoods))
                ) / (-np.log(2) * num_pixels)

            genx = x_hat
            bpp = eval_bpp

            if predict:
                genximg = x_hat_img
                return genx, genximg, bpp, encoding_tensors, gen_metadata
            else:
                return genx, bpp


def generate_dataset(args):
    """
        Generate a dataset generated by the generator from input images from given dataset
        for pre-training a discriminator for GAN training
    :param args: arguments parses by a ArgumentParser
    :return: dataset of generated images
    """
    data_files = [
        os.path.join(args.data_dir, f)
        for f in os.listdir(args.data_dir)
        if f.endswith(".png")
    ]
    print("/// Total images found in the dataset : ", len(data_files))
    json_path = os.path.join(args.model_dir, "params.json")
    assert os.path.isfile(json_path), "No json configuration file found at {}".format(
        json_path
    )
    hparams = HParams(json_path)
    rgb_metric_log_line = ""
    yuv_metric_log_line = ""

    for i, imgfile in enumerate(data_files):
        generatorGraph = tf.Graph()
        with generatorGraph.as_default():
            x = read_png(imgfile)
            x = tf.expand_dims(x, 0)
            x.set_shape([1, None, None, 3])
            num_pixels = tf.cast(tf.reduce_prod(tf.shape(x)[:-1]), dtype=tf.float32)
            x_shape = tf.shape(x)
            x_img = x[0, : x_shape[1], : x_shape[2], :]

            genx, genximg, eval_bpp, encoding_tensors = generator(
                params=hparams, inputs=x, is_training=False, predict=True
            )

            # calculate metrics for the generated images for later analysis

            # Bring both images back to 0..255 range.
            x *= 255
            yuv_x = tf.image.rgb_to_yuv(x)
            genx = tf.clip_by_value(genx, 0, 1)
            yuv_genx = tf.image.rgb_to_yuv(genx)
            genx = tf.round(genx * 255)

            # metrics in RGB colorspace
            eval_mse = tf.reduce_mean(tf.squared_difference(x, genx))
            eval_psnr = tf.squeeze(tf.image.psnr(genx, x, 255))
            # metrics in YUV colorspace
            eval_mse_yuv = tf.reduce_mean(tf.squared_difference(yuv_x, yuv_genx))
            eval_psnr_yuv = tf.squeeze(tf.image.psnr(yuv_x, yuv_genx, 1.0))

            # The following ops are inherently optimized for cpu
            with tf.device("/cpu:0"):
                # metrics in RGB colorspace
                eval_ssim = tf.squeeze(tf.image.ssim(genx, x, 255))
                eval_msssim = tf.squeeze(tf.image.ssim_multiscale(genx, x, 255))
                # metrics in YUV colorspace
                eval_ssim_yuv = tf.squeeze(tf.image.ssim(yuv_genx, yuv_x, 1.0))
                eval_msssim_yuv = tf.squeeze(
                    tf.image.ssim_multiscale(yuv_genx, yuv_x, 1.0)
                )

            with tf.Session() as sess:
                # Load the latest model checkpoint
                # Start the queue runners. If they are not started the program will hang
                # see e.g. https://www.tensorflow.org/programmers_guide/reading_data
                coord = tf.train.Coordinator()
                threads = []
                for qr in sess.graph.get_collection(tf.GraphKeys.QUEUE_RUNNERS):
                    threads.extend(
                        qr.create_threads(sess, coord=coord, daemon=True, start=True)
                    )

                # Load the pretrained model
                print("Loading weights from the pre-trained model")
                if args.use_adversarial_loss:
                    checkpoint_path = os.path.join(args.model_dir, "generator")
                else:
                    checkpoint_path = args.model_dir
                latest = tf.train.latest_checkpoint(checkpoint_dir=checkpoint_path)
                ckpt_iteration = int(latest.split("-")[-1])
                print("Variables loaded from the checkpoint")
                print(
                    "/// The latest Checkpoint was trained for {} iterations >>".format(
                        ckpt_iteration
                    )
                )
                tf.train.Saver().restore(sess, save_path=latest)

                # Write reconstructed image out as a PNG file.
                genpathimg = os.path.join(
                    args.model_dir,
                    "gen_dataset",
                    "{}_{}_comp.png".format(ckpt_iteration, i),
                )
                genx_save_op = write_png(genpathimg, genximg)
                if i < 1:
                    x_save_path = os.path.join(
                        args.model_dir, "{}_{}_gt.png".format(ckpt_iteration, i)
                    )
                    x_save_op = write_png(x_save_path, x_img)
                    # This op saves the 1st ground truth image as a png file
                    sess.run(x_save_op)

                # This op saves the generated image as a png file
                sess.run(genx_save_op)

                # get metrics
                (
                    eval_bpp,
                    eval_mse_yuv,
                    eval_psnr_yuv,
                    eval_ssim_yuv,
                    eval_msssim_yuv,
                    eval_mse,
                    eval_psnr,
                    eval_ssim,
                    eval_msssim,
                    num_pixels,
                ) = sess.run(
                    [
                        eval_bpp,
                        eval_mse_yuv,
                        eval_psnr_yuv,
                        eval_ssim_yuv,
                        eval_msssim_yuv,
                        eval_mse,
                        eval_psnr,
                        eval_ssim,
                        eval_msssim,
                        num_pixels,
                    ]
                )

                rgb_metric_log_line += " {:0.7f}, {:0.7f}, {:0.7f}, {:0.7f}, {:0.7f}".format(
                    eval_bpp, eval_mse, eval_psnr, eval_ssim, eval_msssim
                )
                yuv_metric_log_line += " {:0.7f}, {:0.7f}, {:0.7f}, {:0.7f}".format(
                    eval_mse_yuv, eval_psnr_yuv, eval_ssim_yuv, eval_msssim_yuv
                )
                # Write compression metrics to log
                log_path = os.path.join(args.model_dir, "gen_dataset")
                with open(log_path + "/gen_dataset_metrics.csv", "a") as f:
                    if i == 1:
                        f.write(
                            "ckpt_i, eval_bpp, mse, psnr, ssim, msssim, mse_yuv, psnr_yuv, ssim_yuv, msssim_yuv,\n"
                        )
                    f.write(
                        "%d, %s, %s,\n"
                        % (ckpt_iteration, rgb_metric_log_line, yuv_metric_log_line)
                    )
